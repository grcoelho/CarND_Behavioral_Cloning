{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Angle Detection using Keras - Behaviour Cloning\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset\n",
    "\n",
    "Load the exported dataset from the simulator. \n",
    "The CSV file contains:\n",
    "center,left,right,steering,throttle,brake,speed\n",
    "\n",
    "(3 pictures, steering angle, throttle, brake and speed)\n",
    "\n",
    "Images are all (320,160,3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Modules loaded.\n",
      "Training data downloaded.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "import math\n",
    "import tensorflow as tf\n",
    "# Fix error with TF and Keras\n",
    "tf.python.control_flow_ops = tf\n",
    "\n",
    "print('Modules loaded.')\n",
    "\n",
    "working_file = 'data/data-Original_udacity/driving_log.csv'\n",
    "\n",
    "data = pd.read_csv(working_file)\n",
    "#to Read data do: data.COLUMN.loc[INDEX] (example data.left.loc[2138])\n",
    "#to get the size of this dataset: len(data.index)\n",
    "\n",
    "print('Training data downloaded.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load images into 3 datasets\n",
    "\n",
    "left, center and right. all (160,320,3)\n",
    "\n",
    "\n",
    "Access the dataset: data.COLUMN.loc[INDEX] (example data.left.loc[2138])\n",
    "\n",
    "Lenght of dataset: len(data.index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the Data\n",
    "\n",
    "Import picture per picture for a matrix.\n",
    "To avoid memory issues, we are already normalizing each picture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Center Image:  (8036, 70, 320, 3)\n",
      "All Images Loaded Sucessfully !\n"
     ]
    }
   ],
   "source": [
    "#loading images\n",
    "import matplotlib.image as img\n",
    "\n",
    "def normalize_grayscale(image_data):\n",
    "    \"\"\"\n",
    "    Normalize the image data with Min-Max scaling to a range of [0.1, 0.9]\n",
    "    :param image_data: The image data to be normalized\n",
    "    :return: Normalized image data\n",
    "    \"\"\"\n",
    "    a = -0.5\n",
    "    b = 0.5\n",
    "    grayscale_min = 0\n",
    "    grayscale_max = 255\n",
    "    return a + ( ( (image_data - grayscale_min)*(b - a) )/( grayscale_max - grayscale_min ) )\n",
    "\n",
    "\n",
    "cut_down = 60\n",
    "cut_up = 130\n",
    "\n",
    "##left\n",
    "#left_image = np.zeros((len(data.index), 160,320,3))\n",
    "#for i in range(0,len(data.index)):\n",
    "#    file_name = \"data/data-smooth-1try/IMG/\" + data.left.loc[i]\n",
    "#    left_image[i,:,:,:] = img.imread(file_name)\n",
    "#print(\"Left Image: \",left_image.shape)\n",
    "\n",
    "#Center\n",
    "\n",
    "\n",
    "center_image = np.zeros((len(data.index), 70,320,3))\n",
    "#center_image = np.zeros((100, 160,320,3))\n",
    "for i in range(0,len(data.index)):\n",
    "#for i in range(0,100):    \n",
    "    file_name = \"data/data-Original_udacity/\" + data.center.loc[i]#data.center.loc[i+100]\n",
    "    image = img.imread(file_name)\n",
    "    center_image[i,:,:,:] = image[cut_down:cut_up,:,:]#normalize_grayscale(img.imread(file_name))\n",
    "print(\"Center Image: \",center_image.shape)\n",
    "\n",
    "##Right\n",
    "#right_image = np.zeros((len(data.index), 160,320,3))\n",
    "#for i in range(0,len(data.index)):\n",
    "#    file_name = \"data/data-smooth-1try/IMG/\" + data.right.loc[i]\n",
    "#    right_image[i,:,:,:] = img.imread(file_name)\n",
    "#print(\"Right Image: \",right_image.shape)\n",
    "\n",
    "\n",
    "print(\"All Images Loaded Sucessfully !\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(8036, 70, 320, 3)\n",
      "(8036,)\n",
      "X_train and y_train loaded Sucessfuly !\n"
     ]
    }
   ],
   "source": [
    "#Define the X_train and y_train\n",
    "\n",
    "# X_train will be only defined as the center image for a while\n",
    "X_train = center_image\n",
    "# y_train will be the steering angles\n",
    "limit_max = len(data.index)\n",
    "y_train = np.zeros((limit_max))\n",
    "#y_train = np.zeros((100))\n",
    "for i in range(0,limit_max):\n",
    "#for i in range(0,100):    \n",
    "    y_train[i] = data.steering.loc[i] #data.steering.loc[i+100]\n",
    "\n",
    "print(X_train.shape)\n",
    "print(y_train.shape)\n",
    "\n",
    "print('X_train and y_train loaded Sucessfuly !')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocess the Data\n",
    "\n",
    "1. Shuffle the data\n",
    "2. Normalize the features using Min-Max scaling between -0.5 and 0.5\n",
    "3. One-Hot Encode the labels\n",
    "\n",
    "### Shuffle the data\n",
    "Hint: You can use the [scikit-learn shuffle](http://scikit-learn.org/stable/modules/generated/sklearn.utils.shuffle.html) function to shuffle the data.\n",
    "\n",
    "Shuffle Crashed Jupyter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# TODO: Shuffle the data\n",
    "#from sklearn.utils import shuffle\n",
    "#X_train, y_train = shuffle(X_train,y_train, random_state=0)\n",
    "\n",
    "#print('Data shuffled.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Normalize the features\n",
    "Hint: You solved this in [TensorFlow lab](https://github.com/udacity/CarND-TensorFlow-Lab/blob/master/lab.ipynb) Problem 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# TODO: Normalize the data features to the variable X_normalized\n",
    "\n",
    "def normalize_grayscale(image_data):\n",
    "    \"\"\"\n",
    "    Normalize the image data with Min-Max scaling to a range of [0.1, 0.9]\n",
    "    :param image_data: The image data to be normalized\n",
    "    :return: Normalized image data\n",
    "    \"\"\"\n",
    "    a = -0.5\n",
    "    b = 0.5\n",
    "    grayscale_min = 0\n",
    "    grayscale_max = 255\n",
    "    return a + ( ( (image_data - grayscale_min)*(b - a) )/( grayscale_max - grayscale_min ) )\n",
    "\n",
    "\n",
    "#X_normalized = normalize_grayscale(X_train)\n",
    "X_normalized = X_train\n",
    "#print('Data normalized.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### One-Hot Encode the labels\n",
    "Hint: You can use the [scikit-learn LabelBinarizer](http://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.LabelBinarizer.html) function to one-hot encode the labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "unknown categorical feature present [1 1] during transform.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-023e5d42fa44>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mY_labels\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m \u001b[0my_one_hot\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mOHE_labels\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'y One Hot Encoded.'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-6-023e5d42fa44>\u001b[0m in \u001b[0;36mOHE_labels\u001b[0;34m(Y_tr, N_classes)\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0mY_ohc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mOHC\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mN_classes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m     \u001b[0mY_labels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mY_ohc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mY_tr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mY_labels\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/grcoelho/anaconda3/envs/keras/lib/python3.5/site-packages/sklearn/preprocessing/data.py\u001b[0m in \u001b[0;36mtransform\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m   1956\u001b[0m         \"\"\"\n\u001b[1;32m   1957\u001b[0m         return _transform_selected(X, self._transform,\n\u001b[0;32m-> 1958\u001b[0;31m                                    self.categorical_features, copy=True)\n\u001b[0m",
      "\u001b[0;32m/home/grcoelho/anaconda3/envs/keras/lib/python3.5/site-packages/sklearn/preprocessing/data.py\u001b[0m in \u001b[0;36m_transform_selected\u001b[0;34m(X, transform, selected, copy)\u001b[0m\n\u001b[1;32m   1698\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1699\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mselected\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msix\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstring_types\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mselected\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"all\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1700\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1701\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1702\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mselected\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/grcoelho/anaconda3/envs/keras/lib/python3.5/site-packages/sklearn/preprocessing/data.py\u001b[0m in \u001b[0;36m_transform\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m   1927\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandle_unknown\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'error'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1928\u001b[0m                 raise ValueError(\"unknown categorical feature present %s \"\n\u001b[0;32m-> 1929\u001b[0;31m                                  \"during transform.\" % X.ravel()[~mask])\n\u001b[0m\u001b[1;32m   1930\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1931\u001b[0m         \u001b[0mcolumn_indices\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mindices\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mravel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mmask\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: unknown categorical feature present [1 1] during transform."
     ]
    }
   ],
   "source": [
    "# TODO: One Hot encode the labels to the variable y_one_hot\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "def OHE_labels(Y_tr,N_classes):\n",
    "    OHC = OneHotEncoder()\n",
    "    \n",
    "    Y_ohc = OHC.fit(np.arange(N_classes).reshape(-1, 1))\n",
    "    Y_labels = Y_ohc.transform(Y_tr.reshape(-1, 1)).toarray()\n",
    "    return Y_labels\n",
    "\n",
    "y_one_hot = OHE_labels(y_train,1)\n",
    "\n",
    "print('y One Hot Encoded.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the Network\n",
    "\n",
    "1. Compile the network using adam optimizer and categorical_crossentropy loss function.\n",
    "2. Train the network for ten epochs and validate with 20% of the training data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimization\n",
    "Congratulations! You've built a neural network with convolutions, pooling, dropout, and fully-connected layers, all in just a few lines of code.\n",
    "\n",
    "Have fun with the model and see how well you can do! Add more layers, or regularization, or different padding, or batches, or more training epochs.\n",
    "\n",
    "What is the best validation accuracy you can achieve?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 6428 samples, validate on 1608 samples\n",
      "Epoch 1/10\n",
      "1779s - loss: 0.0140 - mean_absolute_error: 0.0735 - val_loss: 0.0222 - val_mean_absolute_error: 0.1156\n",
      "Epoch 2/10\n",
      "1550s - loss: 0.0108 - mean_absolute_error: 0.0673 - val_loss: 0.0193 - val_mean_absolute_error: 0.1117\n",
      "Epoch 3/10\n",
      "1490s - loss: 0.0096 - mean_absolute_error: 0.0631 - val_loss: 0.0117 - val_mean_absolute_error: 0.0759\n",
      "Epoch 4/10\n",
      "1558s - loss: 0.0091 - mean_absolute_error: 0.0614 - val_loss: 0.0106 - val_mean_absolute_error: 0.0739\n",
      "Epoch 5/10\n",
      "1633s - loss: 0.0088 - mean_absolute_error: 0.0600 - val_loss: 0.0102 - val_mean_absolute_error: 0.0729\n",
      "Epoch 6/10\n",
      "1616s - loss: 0.0085 - mean_absolute_error: 0.0595 - val_loss: 0.0104 - val_mean_absolute_error: 0.0721\n",
      "Epoch 7/10\n",
      "1602s - loss: 0.0082 - mean_absolute_error: 0.0588 - val_loss: 0.0102 - val_mean_absolute_error: 0.0729\n",
      "Epoch 8/10\n",
      "1574s - loss: 0.0084 - mean_absolute_error: 0.0588 - val_loss: 0.0101 - val_mean_absolute_error: 0.0698\n",
      "Epoch 9/10\n",
      "1514s - loss: 0.0080 - mean_absolute_error: 0.0576 - val_loss: 0.0104 - val_mean_absolute_error: 0.0695\n",
      "Epoch 10/10\n",
      "1506s - loss: 0.0077 - mean_absolute_error: 0.0568 - val_loss: 0.0107 - val_mean_absolute_error: 0.0738\n"
     ]
    }
   ],
   "source": [
    "# TODO: Build a model\n",
    "from keras.layers import Convolution2D\n",
    "from keras.layers import MaxPooling2D\n",
    "from keras.layers import Dropout\n",
    "from keras.models import Sequential\n",
    "from keras.layers.core import Dense, Activation, Flatten\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "\n",
    "\n",
    "# Reflections\n",
    "# Dropout doesnt make sense for continous output\n",
    "# As well as Softmax\n",
    "\n",
    "# number of convolutional filters to use\n",
    "nb_filters = 32\n",
    "# convolution kernel size\n",
    "kernel_size = (3, 3)\n",
    "# size of pooling area for max pooling\n",
    "pool_size = (2, 2)\n",
    "\n",
    "\n",
    "# Create the Sequential model\n",
    "model = Sequential()\n",
    "\n",
    "\n",
    "#1nd Layer - Normalization\n",
    "model.add(BatchNormalization(input_shape=(70, 320, 3)))\n",
    "\n",
    "#2nd Layer - Conv\n",
    "model.add(Convolution2D(nb_filters, kernel_size[0], kernel_size[1],\n",
    "                        border_mode='valid'))\n",
    "model.add(MaxPooling2D(pool_size=pool_size, dim_ordering=\"th\"))\n",
    "model.add(Activation('relu'))\n",
    "\n",
    "#3rd Layer - Conv\n",
    "model.add(Convolution2D(nb_filters, kernel_size[0], kernel_size[1],\n",
    "                        border_mode='valid'))\n",
    "model.add(MaxPooling2D(pool_size=pool_size, dim_ordering=\"th\"))\n",
    "model.add(Activation('relu'))\n",
    "\n",
    "#4rd Layer - Conv\n",
    "model.add(Convolution2D(nb_filters, kernel_size[0], kernel_size[1],\n",
    "                        border_mode='valid'))\n",
    "model.add(MaxPooling2D(pool_size=pool_size, dim_ordering=\"th\"))\n",
    "model.add(Activation('relu'))\n",
    "\n",
    "#5th Layer - Conv\n",
    "model.add(Convolution2D(nb_filters, 3, 3, border_mode='valid'))\n",
    "model.add(MaxPooling2D(pool_size=pool_size, dim_ordering=\"th\"))\n",
    "model.add(Activation('relu'))\n",
    "\n",
    "#6th Layer - Conv\n",
    "model.add(Convolution2D(nb_filters, 3, 3, border_mode='valid'))\n",
    "model.add(MaxPooling2D(pool_size=pool_size, dim_ordering=\"th\"))\n",
    "model.add(Activation('relu'))\n",
    "\n",
    "# 7st Layer - Add a flatten layer\n",
    "model.add(Flatten())\n",
    "\n",
    "## 8th Layer - Add a fully connected layer\n",
    "model.add(Dense(100))\n",
    "model.add(Activation('relu'))\n",
    "\n",
    "# 9th Layer - Add a fully connected layer\n",
    "model.add(Dense(50))\n",
    "model.add(Activation('relu'))\n",
    "\n",
    "## 10th Layer - Add a fully connected layer\n",
    "model.add(Dense(10))\n",
    "model.add(Activation('relu'))\n",
    "\n",
    "# 11th Layer - Add a fully connected layer\n",
    "model.add(Dense(1))\n",
    "model.add(Activation('tanh'))\n",
    "\n",
    "# TODO: Compile and train the model\n",
    "model.compile('adam', 'mean_squared_error', ['mean_absolute_error'])\n",
    "history = model.fit(X_train, y_train, nb_epoch=10, validation_split=0.2,batch_size=128,verbose=2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## como obter saida linear entre dois valores ?????????????\n",
    "\n",
    "## Testing\n",
    "Once you've picked out your best model, it's time to test it.\n",
    "\n",
    "Load up the test data and use the [`evaluate()` method](https://keras.io/models/model/#evaluate) to see how well it does.\n",
    "\n",
    "Hint 1: The `evaluate()` method should return an array of numbers. Use the [`metrics_names`](https://keras.io/models/model/) property to get the labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running Prediction ...\n",
      "[[-0.00857567]]\n",
      "Finished\n",
      "0.2531306\n"
     ]
    }
   ],
   "source": [
    "# TODO: Load test data\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "\n",
    "X_test = np.zeros((1, 70,320,3))\n",
    "\n",
    "\n",
    "\n",
    "X_test[0,:,:,:]= X_train[150,:,:,:]\n",
    "#y_test = data_test['labels']\n",
    "\n",
    "# TODO: Preprocess data & one-hot encode the labels\n",
    "#X_test = normalize_grayscale(X_test)\n",
    "#y_one_hot_test = label_binarizer.fit_transform(y_test)\n",
    "#y_one_hot_test = OHE_labels(y_test,43)\n",
    "\n",
    "# Run a Prediction\n",
    "print(\"Running Prediction ...\")\n",
    "predictions = model.predict(X_test)\n",
    "\n",
    "\n",
    "print(predictions)\n",
    "print(\"Finished\")\n",
    "\n",
    "print (y_train[150])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved model to disk\n"
     ]
    }
   ],
   "source": [
    "#Saving\n",
    "#from: http://machinelearningmastery.com/save-load-keras-deep-learning-models/\n",
    "from keras.models import model_from_json\n",
    "\n",
    "# serialize model to JSON\n",
    "model_json = model.to_json()\n",
    "with open(\"drive/model.json\", \"w\") as json_file:\n",
    "    json_file.write(model_json)\n",
    "# serialize weights to HDF5\n",
    "model.save_weights(\"drive/model.h5\")\n",
    "print(\"Saved model to disk\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Loading Model\n",
    "\n",
    "# load json and create model\n",
    "json_file = open('drive/model.json', 'r')\n",
    "loaded_model_json = json_file.read()\n",
    "json_file.close()\n",
    "loaded_model = model_from_json(loaded_model_json)\n",
    "# load weights into new model\n",
    "loaded_model.load_weights(\"drive/model.h5\")\n",
    "print(\"Loaded model from disk\")\n",
    "\n",
    "loaded_model.compile('adam', 'mean_squared_error', ['mean_absolute_error'])\n",
    "\n",
    "# TODO: Load test data\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "\n",
    "X_test = np.zeros((1, 160,320,3))\n",
    "\n",
    "\n",
    "\n",
    "X_test[0,:,:,:]= X_train[2120,:,:,:]\n",
    "#y_test = data_test['labels']\n",
    "\n",
    "# TODO: Preprocess data & one-hot encode the labels\n",
    "#X_test = normalize_grayscale(X_test)\n",
    "#y_one_hot_test = label_binarizer.fit_transform(y_test)\n",
    "#y_one_hot_test = OHE_labels(y_test,43)\n",
    "\n",
    "# Run a Prediction\n",
    "print(\"Running Prediction ...\")\n",
    "predictions = loaded_model.predict(X_test)\n",
    "\n",
    "\n",
    "print(predictions)\n",
    "print(\"Finished\")\n",
    "\n",
    "print (y_train[2120])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
